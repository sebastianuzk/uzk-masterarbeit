# Autonomous Chatbot Agent with RAG Web Scraper

Ein autonomer Chatbot-Agent basierend auf LangChain und LangGraph mit Open-Source-Komponenten und einem umfassenden Web-Scraping-System f√ºr RAG (Retrieval-Augmented Generation).

## Features

### Core Agent Features
- **Autonomer Agent**: Verwendet LangGraph's `create_react_agent` f√ºr intelligente Entscheidungsfindung
- **Ollama Integration**: Vollst√§ndig Open-Source LLM ohne API-Kosten
- **Multiple Tools**: Wikipedia-Suche, Web-Scraping und DuckDuckGo-Websuche
- **Interactive Chat**: Streamlit-basierte Benutzeroberfl√§che
- **Memory Management**: Persistente Konversationshistorie
- **Tool Integration**: Modulare Tool-Architektur f√ºr einfache Erweiterung
- **Privatsph√§re**: Keine externen API-Aufrufe erforderlich

### RAG Web Scraper Features
- **Batch Web Scraping**: Asynchrone Verarbeitung mehrerer URLs
- **Vector Database Integration**: ChromaDB und FAISS Support
- **Data Structure Analysis**: Dynamische Analyse und Optimierung der Datenstrukturen
- **Multiple Output Formats**: JSON, JSONL, Markdown, HTML Reports
- **Quality Metrics**: Vollst√§ndigkeits- und Konsistenz-Analyse
- **CLI Interface**: Vollst√§ndige Kommandozeilen-Steuerung

## Technologie-Stack

- **LLM**: Ollama (lokal gehostet)
- **Framework**: LangChain + LangGraph
- **UI**: Streamlit
- **Suche**: DuckDuckGo (privatsph√§refreundlich)
- **Wissen**: Wikipedia
- **Vector Databases**: ChromaDB, FAISS
- **Embeddings**: Sentence Transformers, OpenAI (optional)
- **Web Scraping**: aiohttp, BeautifulSoup

## Projektstruktur

```
uzk-masterarbeit/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ react_agent.py       # Hauptagent mit LangGraph
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wikipedia_tool.py    # Wikipedia-Suche
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ web_scraper_tool.py  # Web-Scraping f√ºr Agent
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ duckduckgo_tool.py   # DuckDuckGo-Suche
‚îÇ   ‚îú‚îÄ‚îÄ scraper/                 # RAG Web Scraper System
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_scraper.py     # Batch-Webscraper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py      # Vector Database Integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_structure_analyzer.py  # Datenstruktur-Analyse
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper_main.py      # CLI Interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_example.py      # Test & Beispiel-Script
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md            # Scraper-Dokumentation
‚îÇ   ‚îî‚îÄ‚îÄ ui/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ streamlit_app.py     # Streamlit Interface
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ settings.py              # Konfiguration
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ test_tools.py
‚îú‚îÄ‚îÄ Masterarbeit/                # Virtuelles Environment
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt             # Alle Dependencies (Agent + Scraper)
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Installation

### Schritt 1: Voraussetzungen

#### Python 3.8+ installieren
- Windows: [python.org](https://python.org) - "Add to PATH" ausw√§hlen
- Linux: `sudo apt install python3 python3-pip python3-venv`
- Mac: `brew install python3`

#### Ollama installieren (f√ºr LLM)
- **Windows**: Laden Sie Ollama von [ollama.ai](https://ollama.ai) herunter und installieren
- **Linux/Mac**: `curl -fsSL https://ollama.ai/install.sh | sh`

### Schritt 2: Repository klonen
```bash
git clone https://github.com/sebastianuzk/uzk-masterarbeit.git
cd uzk-masterarbeit
```

### Schritt 3: Virtuelle Umgebung erstellen und aktivieren

#### Windows (PowerShell):
```powershell
# Virtuelle Umgebung erstellen
python -m venv Masterarbeit

# Aktivieren
.\Masterarbeit\Scripts\Activate.ps1

# Falls Execution Policy Fehler auftritt:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

#### Linux/Mac:
```bash
# Virtuelle Umgebung erstellen
python3 -m venv Masterarbeit

# Aktivieren
source Masterarbeit/bin/activate
```

### Schritt 4: Abh√§ngigkeiten installieren
```bash
# Alle Dependencies installieren (Agent + RAG Scraper)
pip install --upgrade pip
pip install -r requirements.txt
```

### Schritt 5: Ollama Setup f√ºr LLM

#### 1. Ollama-Server starten:
```bash
# In einem separaten Terminal/Command Prompt
ollama serve
```

#### 2. LLM-Modell herunterladen:
```bash
# Hauptmodell (empfohlen):
ollama pull llama3.1

# Alternative kleinere Modelle:
ollama pull mistral           # Schneller, weniger Ressourcen
ollama pull codellama         # F√ºr Code-Aufgaben
ollama pull llama3.2:1b       # Sehr klein, f√ºr schwache Hardware
```

### Schritt 6: Konfiguration (Optional)
```bash
# Umgebungsvariablen-Datei erstellen
cp .env.example .env

# .env bearbeiten f√ºr benutzerdefinierte Einstellungen:
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.1
```

### üîß Verf√ºgbare Installationsoptionen

#### Minimale Installation (nur Chatbot Agent):
Wenn Sie nur den Chatbot ohne Web-Scraper ben√∂tigen:
```bash
pip install langchain langgraph langchain-community langchain-core langchain-ollama
pip install streamlit python-dotenv duckduckgo-search wikipedia requests
```

#### Vollst√§ndige Installation (Agent + RAG Scraper):
```bash
pip install -r requirements.txt  # Enth√§lt alles
```

#### Erweiterte Installation (mit GPU-Support f√ºr FAISS):
```bash
pip install -r requirements.txt
# GPU-Version von FAISS installieren (falls CUDA verf√ºgbar):
pip uninstall faiss-cpu
pip install faiss-gpu
```

### Ollama Setup

1. **Ollama-Server starten**:
   ```bash
   ollama serve
   ```

2. **Gew√ºnschtes Modell herunterladen**:
   ```bash
   # Hauptmodell:
   ollama pull llama3.1
   
   # Alternative Modelle:
   ollama pull mistral           # Alternative
   ollama pull codellama         # F√ºr Code-Aufgaben
   ```

3. **Optional: Umgebungsvariablen konfigurieren**:
   ```bash
   cp .env.example .env
   ```
   Bearbeiten Sie `.env` f√ºr benutzerdefinierte Einstellungen.

## Konfiguration

Erstellen Sie optional eine `.env` Datei mit folgenden Variablen:

```
# Ollama Konfiguration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1
```

## üéØ Nutzung

### ‚úÖ Installations-Checkliste
Vor der ersten Nutzung pr√ºfen Sie:
- [ ] Ollama ist installiert und l√§uft (`ollama serve`)
- [ ] Ein LLM-Modell ist heruntergeladen (`ollama pull llama3.1`)
- [ ] Virtuelle Umgebung ist aktiviert
- [ ] Alle Dependencies sind installiert (`pip install -r requirements.txt`)

### ü§ñ Chatbot Agent

#### 1. Ollama starten (wichtig!)
**Vor jeder Nutzung** m√ºssen Sie Ollama starten:

```bash
# Terminal 1: Ollama-Server starten
ollama serve

# Terminal 2: Modell herunterladen (falls noch nicht vorhanden)
ollama pull llama3.1
```

#### 2. Agent verwenden

**Streamlit Web-Interface (empfohlen):**
```bash
# VS Code Task verwenden oder direkt:
streamlit run src/ui/streamlit_app.py

# Mit spezifischem Python-Interpreter:
./Masterarbeit/Scripts/python.exe -m streamlit run src/ui/streamlit_app.py
```

**Kommandozeilen-Interface:**
```bash
# VS Code Task verwenden oder direkt:
python main.py

# Mit spezifischem Python-Interpreter:
./Masterarbeit/Scripts/python.exe main.py
```

### üîç RAG Web Scraper (Unabh√§ngiges System)

Das RAG Web Scraper System funktioniert vollst√§ndig unabh√§ngig vom Chatbot.

#### Schnellstart-Beispiele:

**Einzelne Webseite scrapen:**
```bash
python src/scraper/scraper_main.py pipeline \
  --urls https://wiso.uni-koeln.de/de/studium/bewerbung/bachelor \
  --collection uni_bewerbung \
  --save-scraped
```

**Mehrere Webseiten gleichzeitig:**
```bash
python src/scraper/scraper_main.py pipeline \
  --urls https://wiso.uni-koeln.de/de/studium/bewerbung/bachelor \
         https://wiso.uni-koeln.de/de/studium/bewerbung/master \
  --collection uni_infos \
  --save-scraped \
  --concurrent 3
```

**Abfrage an die Vectordatenbank:**
```bash
python src/scraper/scraper_main.py search \
  --query "Was ben√∂tige ich f√ºr die Bewerbung auf ein h√∂heres Fachsemester?" \
  --collection uni_bewerbung \
  --results 5
```

**Chunks der Vectordatenbank anzeigen:**
```bash
python src/scraper/scraper_main.py chunks \
  --collection uni_bewerbung \
  --limit 3 \
  --export json
```

#### Vollst√§ndige Scraper-Dokumentation:
Detaillierte Anweisungen finden Sie in [`src/scraper/README.md`](src/scraper/README.md).

### üß™ Tests ausf√ºhren
```bash
# VS Code Task verwenden oder direkt:
python -m pytest tests/

# Mit spezifischem Python-Interpreter:
./Masterarbeit/Scripts/python.exe -m pytest tests/
```

### üîß VS Code Tasks verwenden

Das Projekt enth√§lt vordefinierte VS Code Tasks:
- **"Start Streamlit App"**: Startet die Web-UI
- **"Run Main Script"**: Startet das CLI-Interface  
- **"Run Tests"**: F√ºhrt alle Tests aus

Zugriff √ºber: `Ctrl+Shift+P` ‚Üí "Tasks: Run Task"

## Features im Detail

### React Agent
Der Agent verwendet LangGraph's `create_react_agent` Funktionalit√§t f√ºr:
- Reasoning √ºber verf√ºgbare Tools
- Entscheidungsfindung basierend auf Benutzereingaben
- Iterative Probleml√∂sung
- Memory Management f√ºr Kontext

### Verf√ºgbare Tools
1. **Wikipedia Tool**: Suche nach Informationen in Wikipedia
2. **Web Scraper Tool**: Extrahierung von Inhalten aus Webseiten (f√ºr Agent)
3. **DuckDuckGo Tool**: Privatsph√§refreundliche Websuche ohne Tracking

### RAG Web Scraper System
Das separate Scraper-System bietet:
- **Batch Processing**: Verarbeitung vieler URLs parallel
- **Vector Storage**: Speicherung in ChromaDB oder FAISS f√ºr RAG
- **Data Analysis**: Automatische Qualit√§ts- und Strukturanalyse
- **Optimization**: Vorschl√§ge zur Datenverbesserung
- **Flexible Export**: Verschiedene Ausgabeformate

### Memory Management
- Persistente Konversationshistorie
- Kontextuelle Speicherung f√ºr bessere Antworten
- Konfigurierbare Memory-Gr√∂√üe

## Integration von Agent und Scraper

Das RAG System kann sp√§ter in den Chatbot-Agent integriert werden:

1. **Daten sammeln** mit dem Batch Scraper
2. **Vektorisieren** der Inhalte f√ºr semantische Suche
3. **RAG Tool erstellen** f√ºr den Agent mit Zugriff auf die Vector Database
4. **Agent erweitern** um das neue RAG Tool

## üõ†Ô∏è Entwicklung

### üß™ Tests ausf√ºhren
```bash
# Alle Tests
python -m pytest tests/

# Mit Ausgabe
python -m pytest tests/ -v

# Spezifische Testdatei
python -m pytest tests/test_agent.py
```

### üîß Agent erweitern

#### Neue Tools hinzuf√ºgen:
1. Erstellen Sie eine neue Datei in `src/tools/` (z.B. `my_new_tool.py`)
2. Implementieren Sie die Tool-Klasse basierend auf LangChain's `BaseTool`
3. Registrieren Sie das Tool in `src/agent/react_agent.py`

Beispiel:
```python
# src/tools/my_new_tool.py
from langchain.tools import BaseTool

class MyNewTool(BaseTool):
    name = "my_new_tool"
    description = "Beschreibung des Tools"
    
    def _run(self, query: str) -> str:
        # Tool-Logik hier
        return "Ergebnis"
```

#### RAG Tool f√ºr Agent erstellen:
Nach dem Aufbau einer Vectordatenbank mit dem Scraper k√∂nnen Sie ein RAG Tool erstellen:

```python
# src/tools/rag_tool.py
from langchain.tools import BaseTool
from src.scraper.vector_store import VectorStore

class RAGTool(BaseTool):
    name = "knowledge_search"
    description = "Durchsucht die lokale Wissensdatenbank"
    
    def _run(self, query: str) -> str:
        vector_store = VectorStore()
        results = vector_store.search(query, k=3)
        return "\\n".join([doc.text for doc, score in results])
```

### üîç Scraper erweitern

#### Neue Vector Store Backends:
1. Implementieren Sie `VectorStoreBackend` in `src/scraper/vector_store.py`
2. Registrieren Sie das Backend in der `VectorStore` Klasse

#### Zus√§tzliche Datenanalyse:
1. Erweitern Sie `DataStructureAnalyzer` in `src/scraper/data_structure_analyzer.py`
2. Neue CLI-Befehle in `src/scraper/scraper_main.py` hinzuf√ºgen

### üìù Konfiguration anpassen

#### Agent-Konfiguration (`config/settings.py`):
```python
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.1"
MEMORY_KEY = "chat_history"
```

#### Scraper-Konfiguration:
Siehe `src/scraper/hyperparameters.py` f√ºr alle verf√ºgbaren Parameter.

### üêõ Debug-Tipps

#### Ollama Probleme:
```bash
# Ollama Status pr√ºfen
ollama list

# Modell erneut herunterladen
ollama pull llama3.1

# Ollama Logs anzeigen (falls verf√ºgbar)
ollama logs
```

#### Scraper Probleme:
```bash
# Verbose-Modus aktivieren
python src/scraper/scraper_main.py --verbose pipeline --urls https://example.com

# Einzelne Schritte testen
python src/scraper/test_example.py
```

#### Virtual Environment Probleme:
```bash
# Environment neu erstellen
deactivate
rm -rf Masterarbeit  # oder Remove-Item -Recurse Masterarbeit (Windows)
python -m venv Masterarbeit
source Masterarbeit/bin/activate  # oder .\\Masterarbeit\\Scripts\\Activate.ps1
pip install -r requirements.txt
```

## Lizenz

MIT License

**Aktivieren der venv:**
```powershell
# Windows PowerShell
& "D:/Uni-K√∂ln/Masterarbeit/Software/uzk-masterarbeit/Masterarbeit/Scripts/Activate.ps1"

# Dann Streamlit starten:
streamlit run src/ui/streamlit_app.py
```

**Probleml√∂sung bei venv-Konflikten:**
Falls Sie Probleme haben, verwenden Sie diesen direkten Befehl:
```powershell
cd "d:\Uni-K√∂ln\Masterarbeit\Software\uzk-masterarbeit"
.\Masterarbeit\Scripts\python.exe -m streamlit run src/ui/streamlit_app.py
```