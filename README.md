# Autonomer Chatbot-Agent mit RAG Web Scraper

Ein autonomer Chatbot-Agent fÃ¼r die WiSo-FakultÃ¤t der UniversitÃ¤t zu KÃ¶ln, basierend auf LangChain und LangGraph mit Open-Source-Komponenten und einem erweiterten Web-Scraping-System fÃ¼r RAG (Retrieval-Augmented Generation).

## ğŸ¯ Ãœberblick

Dieses Projekt bietet einen intelligenten Chatbot, der:
- âœ… **Fragen zur WiSo-FakultÃ¤t beantwortet** (StudiengÃ¤nge, Bewerbung, Services, etc.)
- âœ… **Automatisch relevante Informationen** aus der FakultÃ¤ts-Website sammelt
- âœ… **Intelligent kategorisiert** (5 Kategorien: Studium, FakultÃ¤t, Services, Forschung, Allgemein)
- âœ… **VollstÃ¤ndig Open-Source** ohne externe API-Kosten arbeitet
- âœ… **Lokal lÃ¤uft** fÃ¼r maximale PrivatsphÃ¤re

## âœ¨ Hauptfunktionen

### Chatbot-Agent
- **Autonomer Agent**: LangGraph's `create_react_agent` fÃ¼r intelligente Entscheidungsfindung
- **Ollama Integration**: VollstÃ¤ndig Open-Source LLM (llama3.1) ohne API-Kosten
- **UniversitÃ¤ts-RAG**: Durchsucht 329 kategorisierte Dokumente der WiSo-FakultÃ¤t
- **Multiple Tools**: Web-Scraping, DuckDuckGo-Suche, KLIPS2-Registrierung, E-Mail-Eskalation
- **KLIPS2-Integration** (NEU): UnterstÃ¼tzt Benutzer bei der Erstellung von Basis-Accounts
- **Streamlit UI**: Moderne, benutzerfreundliche Chat-OberflÃ¤che
- **Konversations-Memory**: Persistente Chat-Historie

### Erweiterter Web Scraper (NEU)
- **Intelligente Kategorisierung**: Automatische Zuordnung zu 5 Kategorien
- **Multi-Collection Vector DB**: Separate ChromaDB-Collections pro Kategorie
- **Metadaten-Anreicherung**: 10+ Metadatenfelder pro Dokument
- **Batch Processing**: Asynchrone Verarbeitung mehrerer URLs
- **QualitÃ¤tsmetriken**: VollstÃ¤ndige Analyse und Reporting
- **329 Dokumente**: 50 Seiten, 100% Erfolgsrate

## ğŸ“Š Daten-Status

```
âœ… 50 Webseiten erfolgreich gescraped
âœ… 329 Dokument-Chunks in Vector-Datenbank
âœ… 5 intelligente Kategorien:
   â€¢ wiso_studium (95 Dokumente)      - StudiengÃ¤nge, Bewerbung
   â€¢ wiso_fakultaet (117 Dokumente)   - Struktur, Departments
   â€¢ wiso_services (61 Dokumente)     - IT, Support, Beratung
   â€¢ wiso_forschung (46 Dokumente)    - Forschungsprojekte
   â€¢ wiso_allgemein (10 Dokumente)    - Sonstiges
```

## ğŸ› ï¸ Technologie-Stack

- **LLM**: Ollama (llama3.1, lokal gehostet)
- **Framework**: LangChain + LangGraph
- **UI**: Streamlit
- **Suche**: DuckDuckGo (privatsphÃ¤refreundlich)
- **Vector Databases**: ChromaDB, FAISS
- **Embeddings**: Sentence Transformers, OpenAI (optional)
- **Vector DB**: ChromaDB mit sentence-transformers
- **Embeddings**: all-MiniLM-L6-v2 (384 Dimensionen)
- **Web Scraping**: aiohttp, BeautifulSoup
- **Suche**: DuckDuckGo, Wikipedia

## ğŸ“ Projektstruktur

```
uzk-masterarbeit/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â””â”€â”€ react_agent.py              # LangGraph ReAct Agent
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ rag_tool.py                 # RAG fÃ¼r WiSo-FakultÃ¤t â­
â”‚   â”‚   â”œâ”€â”€ web_scraper_tool.py         # Web-Scraping Tool
â”‚   â”‚   â”œâ”€â”€ duckduckgo_tool.py          # DuckDuckGo-Suche
â”‚   â”‚   â”œâ”€â”€ klips2_register_tool.py     # KLIPS2-Registrierung â­
â”‚   â”‚   â””â”€â”€ email_tool.py               # E-Mail Support-Eskalation
â”‚   â”œâ”€â”€ scraper/                        # Erweiterte Web Scraper Pipeline â­
â”‚   â”‚   â”œâ”€â”€ core/                       # Kern-Komponenten
â”‚   â”‚   â”‚   â”œâ”€â”€ batch_scraper.py        # Batch-Verarbeitung
â”‚   â”‚   â”‚   â”œâ”€â”€ wiso_crawler.py         # WiSo-Website Crawler
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py         # Vector DB Integration
â”‚   â”‚   â”‚   â”œâ”€â”€ incremental_scraper.py  # Inkrementelles Scraping
â”‚   â”‚   â”‚   â””â”€â”€ resilient_scraper.py    # Fehlertolerantes Scraping
â”‚   â”‚   â”œâ”€â”€ pipelines/                  # AusfÃ¼hrbare Workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler_scraper_pipeline.py  # Haupt-Pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ scraper_main.py         # Scraper Entry Point
â”‚   â”‚   â”‚   â””â”€â”€ reprocess_existing_data.py   # Daten-Wiederaufbereitung
â”‚   â”‚   â”œâ”€â”€ utils/                      # Hilfsfunktionen
â”‚   â”‚   â”‚   â”œâ”€â”€ content_cleaner.py      # Content-Bereinigung
â”‚   â”‚   â”‚   â”œâ”€â”€ content_deduplicator.py # Duplikat-Erkennung
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py        # PDF-Verarbeitung
â”‚   â”‚   â”‚   â”œâ”€â”€ semantic_chunker.py     # Intelligentes Chunking
â”‚   â”‚   â”‚   â””â”€â”€ url_cache.py            # URL-Caching
â”‚   â”‚   â”œâ”€â”€ analysis/                   # Analyse & Monitoring
â”‚   â”‚   â”‚   â”œâ”€â”€ show_cached_urls.py     # Cache-Viewer
â”‚   â”‚   â”‚   â””â”€â”€ scraper_metrics.py      # Metriken & Reports
â”‚   â”‚   â””â”€â”€ hyperparameters.py          # Zentrale Konfiguration
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ streamlit_app.py            # Chat-Interface
â”‚   â””â”€â”€ dev/                            # Entwicklungs-Skripte
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                     # Globale Einstellungen
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vector_db/                      # ChromaDB Collections â­
â”‚   â”œâ”€â”€ url_cache.db                    # URL-Cache SQLite
â”‚   â”œâ”€â”€ pdfs/                           # Heruntergeladene PDFs
â”‚   â””â”€â”€ *.json                          # Metrics & Reports
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_agent.py                   # Agent-Tests
â”‚   â”œâ”€â”€ test_tools.py                   # Tool-Tests
â”‚   â”œâ”€â”€ test_scraper.py                 # Scraper-Tests
â”‚   â”œâ”€â”€ test_scraper_components.py      # Komponenten-Tests
â”‚   â””â”€â”€ test_enhanced_pipeline.py       # Pipeline-Tests
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md         # GitHub Copilot Instruktionen
â”œâ”€â”€ .venv/                              # Virtual Environment
â”œâ”€â”€ .env                                # Umgebungsvariablen (lokal)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt                    # Python Dependencies
â”œâ”€â”€ setup.py                            # Package Setup
â””â”€â”€ README.md
```

## ğŸš€ Schnellstart

### Voraussetzungen
- Python 3.8+
- Ollama installiert und laufend
- 4GB+ RAM empfohlen

### Installation in 5 Minuten

```bash
# 1. Repository klonen
git clone https://github.com/sebastianuzk/uzk-masterarbeit.git
cd uzk-masterarbeit

# 2. Virtuelle Umgebung erstellen
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows

# 3. Dependencies installieren
pip install --upgrade pip
pip install -r requirements.txt

# 4. Ollama-Modell laden (in separatem Terminal)
ollama pull llama3.1:8b

# 5. Chatbot starten
streamlit run src/ui/streamlit_app.py
```

### Erste Schritte

Nach dem Start kÃ¶nnen Sie Fragen stellen wie:
- "Welche Master-Programme bietet die WiSo-FakultÃ¤t an?"
- "Wie bewerbe ich mich fÃ¼r ein hÃ¶heres Fachsemester?"
- "Wo finde ich IT-Support an der WiSo?"
- "Welche Forschungsschwerpunkte gibt es?"

## ğŸ’¡ Verwendung

### Chatbot starten
```bash
streamlit run src/ui/streamlit_app.py
```
Ã–ffnet http://localhost:8501 im Browser.

### Pipeline ausfÃ¼hren (Daten aktualisieren)
```bash
# WiSo-Website scrapen und kategorisieren
python src/scraper/crawler_scraper_pipeline.py --organize-by-category

# Vorhandene Daten wiederaufbereiten
python src/scraper/reprocess_existing_data.py --organize-by-category
```

### CLI-Modus (ohne UI)
```bash
python main.py
```

### Tests ausfÃ¼hren
```bash
# Pipeline-Tests
python test_enhanced_pipeline.py

# Unit-Tests
pytest tests/
```

##  Konfiguration

### Ollama-Einstellungen
Bearbeiten Sie `config/settings.py`:
```python
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.1:8b"  # oder mistral, llama3.2, etc.
TEMPERATURE = 0.7
```

### Scraper-Hyperparameter
Bearbeiten Sie `src/scraper/hyperparameters.py`:
```python
# Performance
SCRAPER_MAX_CONCURRENT_REQUESTS = 10
SCRAPER_REQUEST_DELAY = 1.0

# Vector Store
VECTOR_CHUNK_SIZE = 1500
VECTOR_CHUNK_OVERLAP = 300
VECTOR_EMBEDDING_MODEL = "all-MiniLM-L6-v2"
```

## ğŸ¯ Beispiel-Anfragen

### Studium
```
"Welche Bachelor-Programme gibt es?"
"Wie ist das Master-Programm strukturiert?"
"Was sind Double Degree Programme?"
```

### Bewerbung
```
"Wie bewerbe ich mich fÃ¼r ein hÃ¶heres Fachsemester?"
"Welche Fristen muss ich beachten?"
"Was sind die Zulassungsvoraussetzungen fÃ¼r Master?"
```

### Services
```
"Wo finde ich IT-Support?"
"Welche Beratungsangebote gibt es?"
"Wie erreiche ich das PrÃ¼fungsamt?"
```

### FakultÃ¤t & Forschung
```
"Welche Departments hat die WiSo-FakultÃ¤t?"
"Welche Forschungsschwerpunkte gibt es?"
"Wie ist die FakultÃ¤tsverwaltung organisiert?"
```

## ğŸ› ï¸ Erweiterte Features

### VerfÃ¼gbare Tools

Der Chatbot verfÃ¼gt Ã¼ber folgende intelligente Tools:

#### 1. **UniversitÃ¤ts-RAG-Tool** ğŸ“š
- Durchsucht 329 kategorisierte WiSo-Dokumente
- 5 Kategorien: Studium, FakultÃ¤t, Services, Forschung, Allgemein
- Kontextbasierte Antworten mit Quellenangaben

#### 2. **Web-Scraping-Tool** ğŸŒ
- Extrahiert Inhalte von beliebigen Webseiten
- Automatische Text-Bereinigung
- FÃ¼r aktuelle Informationen auÃŸerhalb der Wissensdatenbank

#### 3. **DuckDuckGo-Suche** ğŸ”
- PrivatsphÃ¤refreundliche Websuche
- FÃ¼r allgemeine Internetrecherche
- Keine Tracking-Cookies

#### 4. **KLIPS2-Registrierungs-Tool** âœ… (NEU)
- UnterstÃ¼tzt bei der Erstellung von Basis-Accounts
- Validiert Eingabedaten (Datum, E-Mail, etc.)
- Gibt strukturierte Anleitungen zur manuellen Registrierung
- Siehe: [KLIPS2_REGISTRATION_TOOL.md](docs/KLIPS2_REGISTRATION_TOOL.md)

#### 5. **E-Mail-Support-Eskalation** ğŸ“§
- Automatische Weiterleitung komplexer Anfragen
- SMTP-Integration fÃ¼r professionellen Support
- Siehe: [EMAIL_SETUP.md](docs/EMAIL_SETUP.md)

### Web Scraper Pipeline

Die erweiterte Pipeline bietet:
- âœ… **Intelligente Kategorisierung**: 8 Kategorien-Muster
- âœ… **Metadaten-Anreicherung**: Sprache, Themen, QualitÃ¤t
- âœ… **Multi-Collection DB**: Separate Collections pro Kategorie
- âœ… **Batch-Processing**: Asynchrone URL-Verarbeitung
- âœ… **QualitÃ¤tsprÃ¼fung**: Automatische Validierung

```bash
# Standard-Pipeline mit Kategorisierung
python src/scraper/crawler_scraper_pipeline.py --organize-by-category

# Erweiterte Optionen
python src/scraper/crawler_scraper_pipeline.py \
  --max-pages 2000 \
  --concurrent-requests 20 \
  --crawl-delay 0.5 \
  --organize-by-category
```

### RAG Tool direkt verwenden

```python
from src.tools.rag_tool import UniversityRAGTool

tool = UniversityRAGTool()
result = tool._run("Wie bewerbe ich mich fÃ¼r Master?")
print(result)
```

### Vector-Datenbank Status prÃ¼fen

```python
import chromadb
from pathlib import Path

client = chromadb.PersistentClient(path='data/vector_db')
collections = client.list_collections()

for c in collections:
    print(f'{c.name}: {c.count()} Dokumente')
```

## ğŸ” Fehlerbehebung

### Ollama nicht erreichbar
```bash
# PrÃ¼fen ob Ollama lÃ¤uft
ollama list

# Ollama starten
ollama serve
```

### Keine Vector-Datenbank gefunden
```bash
# Pipeline ausfÃ¼hren um Daten zu erstellen
python src/scraper/crawler_scraper_pipeline.py --organize-by-category
```

### Import-Fehler
```bash
# Sicherstellen dass virtuelle Umgebung aktiviert ist
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Dependencies erneut installieren
pip install -r requirements.txt
```

### Langsame Performance
- Kleineres Ollama-Modell verwenden: `ollama pull llama3.2:1b`
- Weniger concurrent requests: `--concurrent-requests 5`
- GrÃ¶ÃŸere Delays: `--crawl-delay 2.0`

## ğŸ“ˆ Performance-Metriken

| Metrik | Wert |
|--------|------|
| Gescrapte Seiten | 50 |
| Dokument-Chunks | 329 |
| Collections | 5 |
| Erfolgsrate | 100% |
| Durchschn. Antwortzeit | < 1 Sekunde |
| Embedding-Dimensionen | 384 |
| Pipeline-Laufzeit | ~30 Sekunden |

## ğŸ” Datenschutz

- âœ… Alle Daten werden lokal verarbeitet
- âœ… Kein Senden von Daten an externe APIs
- âœ… Ollama LLM lÃ¤uft vollstÃ¤ndig lokal
- âœ… Vector-Datenbank auf lokalem Dateisystem
- âœ… Keine Telemetrie oder Tracking

## ğŸ¤ Beitragen

Dieses Projekt ist Teil einer Masterarbeit an der UniversitÃ¤t zu KÃ¶ln.

## ğŸ“„ Lizenz

Dieses Projekt ist fÃ¼r akademische Zwecke erstellt.

## ğŸ™ Danksagungen

- WiSo-FakultÃ¤t, UniversitÃ¤t zu KÃ¶ln
- LangChain & LangGraph Teams
- Ollama Team
- Open-Source Community

---

**Version**: 2.0  
**Letztes Update**: Januar 2025  
**Status**: âœ… Produktionsbereit  
**Daten**: 329 kategorisierte Dokumente aus 50 WiSo-Seiten