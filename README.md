# Autonomer Chatbot-Agent mit RAG Web Scraper

Ein autonomer Chatbot-Agent fÃ¼r die WiSo-FakultÃ¤t der UniversitÃ¤t zu KÃ¶ln, basierend auf LangChain und LangGraph mit Open-Source-Komponenten und einem erweiterten Web-Scraping-System fÃ¼r RAG (Retrieval-Augmented Generation).

## ğŸ¯ Ãœberblick

Dieses Projekt bietet einen intelligenten Chatbot, der:
- âœ… **Fragen zur WiSo-FakultÃ¤t beantwortet** (StudiengÃ¤nge, Bewerbung, Services, etc.)
- âœ… **Automatisch relevante Informationen** aus der FakultÃ¤ts-Website sammelt
- âœ… **Intelligent kategorisiert** (5 Kategorien: Studium, FakultÃ¤t, Services, Forschung, Allgemein)
- âœ… **VollstÃ¤ndig Open-Source** ohne externe API-Kosten arbeitet
- âœ… **Lokal lÃ¤uft** fÃ¼r maximale PrivatsphÃ¤re

## âœ¨ Hauptfunktionen

### Chatbot-Agent
- **Autonomer Agent**: LangGraph's `create_react_agent` fÃ¼r intelligente Entscheidungsfindung
- **Ollama Integration**: VollstÃ¤ndig Open-Source LLM (llama3.1) ohne API-Kosten
- **UniversitÃ¤ts-RAG**: Durchsucht 329 kategorisierte Dokumente der WiSo-FakultÃ¤t
- **Multiple Tools**: Wikipedia, Web-Scraping, DuckDuckGo-Suche
- **Streamlit UI**: Moderne, benutzerfreundliche Chat-OberflÃ¤che
- **Konversations-Memory**: Persistente Chat-Historie

### Erweiterter Web Scraper (NEU)
- **Intelligente Kategorisierung**: Automatische Zuordnung zu 5 Kategorien
- **Multi-Collection Vector DB**: Separate ChromaDB-Collections pro Kategorie
- **Metadaten-Anreicherung**: 10+ Metadatenfelder pro Dokument
- **Batch Processing**: Asynchrone Verarbeitung mehrerer URLs
- **QualitÃ¤tsmetriken**: VollstÃ¤ndige Analyse und Reporting
- **329 Dokumente**: 50 Seiten, 100% Erfolgsrate

## ğŸ“Š Daten-Status

```
âœ… 50 Webseiten erfolgreich gescraped
âœ… 329 Dokument-Chunks in Vector-Datenbank
âœ… 5 intelligente Kategorien:
   â€¢ wiso_studium (95 Dokumente)      - StudiengÃ¤nge, Bewerbung
   â€¢ wiso_fakultaet (117 Dokumente)   - Struktur, Departments
   â€¢ wiso_services (61 Dokumente)     - IT, Support, Beratung
   â€¢ wiso_forschung (46 Dokumente)    - Forschungsprojekte
   â€¢ wiso_allgemein (10 Dokumente)    - Sonstiges
```

## ğŸ› ï¸ Technologie-Stack

- **LLM**: Ollama (llama3.1, lokal gehostet)
- **Framework**: LangChain + LangGraph
- **UI**: Streamlit
- **Vector DB**: ChromaDB mit sentence-transformers
- **Embeddings**: all-MiniLM-L6-v2 (384 Dimensionen)
- **Web Scraping**: aiohttp, BeautifulSoup
- **Suche**: DuckDuckGo, Wikipedia

## ğŸ“ Projektstruktur

```
uzk-masterarbeit/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â””â”€â”€ react_agent.py           # LangGraph ReAct Agent
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ rag_tool.py              # RAG fÃ¼r WiSo-FakultÃ¤t â­
â”‚   â”‚   â”œâ”€â”€ wikipedia_tool.py        # Wikipedia-Suche
â”‚   â”‚   â”œâ”€â”€ web_scraper_tool.py      # Web-Scraping
â”‚   â”‚   â””â”€â”€ duckduckgo_tool.py       # DuckDuckGo-Suche
â”‚   â”œâ”€â”€ scraper/                     # Erweiterte Web Scraper Pipeline â­
â”‚   â”‚   â”œâ”€â”€ crawler_scraper_pipeline.py  # Haupt-Pipeline
â”‚   â”‚   â”œâ”€â”€ wiso_crawler.py          # WiSo-Website Crawler
â”‚   â”‚   â”œâ”€â”€ batch_scraper.py         # Batch-Scraper
â”‚   â”‚   â”œâ”€â”€ vector_store.py          # Vector DB Integration
â”‚   â”‚   â”œâ”€â”€ reprocess_existing_data.py   # Daten-Wiederaufbereitung
â”‚   â”‚   â”œâ”€â”€ hyperparameters.py       # Zentrale Konfiguration
â”‚   â”‚   â””â”€â”€ data_analysis/           # Gescrapte Daten & Reports
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ streamlit_app.py         # Chat-Interface
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                  # Globale Einstellungen
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_db/                   # ChromaDB Collections â­
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py                    # Test-Suite
â”œâ”€â”€ main.py                          # CLI-Einstiegspunkt
â”œâ”€â”€ test_enhanced_pipeline.py        # Pipeline-Tests
â””â”€â”€ requirements.txt                 # Alle Dependencies
```

## ğŸš€ Schnellstart

### Voraussetzungen
- Python 3.10+
- Ollama installiert und laufend
- 4GB+ RAM empfohlen

### Installation in 5 Minuten

```bash
# 1. Repository klonen
git clone https://github.com/sebastianuzk/uzk-masterarbeit.git
cd uzk-masterarbeit

# 2. Virtuelle Umgebung erstellen
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# oder
venv\Scripts\activate     # Windows

# 3. Dependencies installieren
pip install --upgrade pip
pip install -r requirements.txt

# 4. Ollama-Modell laden (in separatem Terminal)
ollama pull llama3.1:8b

# 5. Chatbot starten
streamlit run src/ui/streamlit_app.py
```

### Erste Schritte

Nach dem Start kÃ¶nnen Sie Fragen stellen wie:
- "Welche Master-Programme bietet die WiSo-FakultÃ¤t an?"
- "Wie bewerbe ich mich fÃ¼r ein hÃ¶heres Fachsemester?"
- "Wo finde ich IT-Support an der WiSo?"
- "Welche Forschungsschwerpunkte gibt es?"

## ğŸ’¡ Verwendung

### Chatbot starten
```bash
streamlit run src/ui/streamlit_app.py
```
Ã–ffnet http://localhost:8501 im Browser.

### Pipeline ausfÃ¼hren (Daten aktualisieren)
```bash
# WiSo-Website scrapen und kategorisieren
python src/scraper/crawler_scraper_pipeline.py --organize-by-category

# Vorhandene Daten wiederaufbereiten
python src/scraper/reprocess_existing_data.py --organize-by-category
```

### CLI-Modus (ohne UI)
```bash
python main.py
```

### Tests ausfÃ¼hren
```bash
# Pipeline-Tests
python test_enhanced_pipeline.py

# Unit-Tests
pytest tests/
```

##  Konfiguration

### Ollama-Einstellungen
Bearbeiten Sie `config/settings.py`:
```python
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.1:8b"  # oder mistral, llama3.2, etc.
TEMPERATURE = 0.7
```

### Scraper-Hyperparameter
Bearbeiten Sie `src/scraper/hyperparameters.py`:
```python
# Performance
SCRAPER_MAX_CONCURRENT_REQUESTS = 10
SCRAPER_REQUEST_DELAY = 1.0

# Vector Store
VECTOR_CHUNK_SIZE = 1500
VECTOR_CHUNK_OVERLAP = 300
VECTOR_EMBEDDING_MODEL = "all-MiniLM-L6-v2"
```

## ğŸ¯ Beispiel-Anfragen

### Studium
```
"Welche Bachelor-Programme gibt es?"
"Wie ist das Master-Programm strukturiert?"
"Was sind Double Degree Programme?"
```

### Bewerbung
```
"Wie bewerbe ich mich fÃ¼r ein hÃ¶heres Fachsemester?"
"Welche Fristen muss ich beachten?"
"Was sind die Zulassungsvoraussetzungen fÃ¼r Master?"
```

### Services
```
"Wo finde ich IT-Support?"
"Welche Beratungsangebote gibt es?"
"Wie erreiche ich das PrÃ¼fungsamt?"
```

### FakultÃ¤t & Forschung
```
"Welche Departments hat die WiSo-FakultÃ¤t?"
"Welche Forschungsschwerpunkte gibt es?"
"Wie ist die FakultÃ¤tsverwaltung organisiert?"
```

## ğŸ› ï¸ Erweiterte Features

### Web Scraper Pipeline

Die erweiterte Pipeline bietet:
- âœ… **Intelligente Kategorisierung**: 8 Kategorien-Muster
- âœ… **Metadaten-Anreicherung**: Sprache, Themen, QualitÃ¤t
- âœ… **Multi-Collection DB**: Separate Collections pro Kategorie
- âœ… **Batch-Processing**: Asynchrone URL-Verarbeitung
- âœ… **QualitÃ¤tsprÃ¼fung**: Automatische Validierung

```bash
# Standard-Pipeline mit Kategorisierung
python src/scraper/crawler_scraper_pipeline.py --organize-by-category

# Erweiterte Optionen
python src/scraper/crawler_scraper_pipeline.py \
  --max-pages 2000 \
  --concurrent-requests 20 \
  --crawl-delay 0.5 \
  --organize-by-category
```

### RAG Tool direkt verwenden

```python
from src.tools.rag_tool import UniversityRAGTool

tool = UniversityRAGTool()
result = tool._run("Wie bewerbe ich mich fÃ¼r Master?")
print(result)
```

### Vector-Datenbank Status prÃ¼fen

```python
import chromadb
from pathlib import Path

client = chromadb.PersistentClient(path='data/vector_db')
collections = client.list_collections()

for c in collections:
    print(f'{c.name}: {c.count()} Dokumente')
```

## ğŸ” Fehlerbehebung

### Ollama nicht erreichbar
```bash
# PrÃ¼fen ob Ollama lÃ¤uft
ollama list

# Ollama starten
ollama serve
```

### Keine Vector-Datenbank gefunden
```bash
# Pipeline ausfÃ¼hren um Daten zu erstellen
python src/scraper/crawler_scraper_pipeline.py --organize-by-category
```

### Import-Fehler
```bash
# Sicherstellen dass virtuelle Umgebung aktiviert ist
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Dependencies erneut installieren
pip install -r requirements.txt
```

### Langsame Performance
- Kleineres Ollama-Modell verwenden: `ollama pull llama3.2:1b`
- Weniger concurrent requests: `--concurrent-requests 5`
- GrÃ¶ÃŸere Delays: `--crawl-delay 2.0`

## ğŸ“ˆ Performance-Metriken

| Metrik | Wert |
|--------|------|
| Gescrapte Seiten | 50 |
| Dokument-Chunks | 329 |
| Collections | 5 |
| Erfolgsrate | 100% |
| Durchschn. Antwortzeit | < 1 Sekunde |
| Embedding-Dimensionen | 384 |
| Pipeline-Laufzeit | ~30 Sekunden |

## ğŸ” Datenschutz

- âœ… Alle Daten werden lokal verarbeitet
- âœ… Kein Senden von Daten an externe APIs
- âœ… Ollama LLM lÃ¤uft vollstÃ¤ndig lokal
- âœ… Vector-Datenbank auf lokalem Dateisystem
- âœ… Keine Telemetrie oder Tracking

## ğŸ¤ Beitragen

Dieses Projekt ist Teil einer Masterarbeit an der UniversitÃ¤t zu KÃ¶ln.

## ğŸ“„ Lizenz

Dieses Projekt ist fÃ¼r akademische Zwecke erstellt.

## ğŸ™ Danksagungen

- WiSo-FakultÃ¤t, UniversitÃ¤t zu KÃ¶ln
- LangChain & LangGraph Teams
- Ollama Team
- Open-Source Community

---

**Version**: 2.0  
**Letztes Update**: Januar 2025  
**Status**: âœ… Produktionsbereit  
**Daten**: 329 kategorisierte Dokumente aus 50 WiSo-Seiten