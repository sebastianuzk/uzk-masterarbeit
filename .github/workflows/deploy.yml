name: Test Chatbot

on:
  push:
    branches:
      - main
      - "feat/**" # Alle Feature-Branches
  pull_request:
    branches:
      - main

env:
  PYTHON_VERSION: "3.11"

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest

    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        options: >-
          --health-cmd "ollama list || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"
          cache-dependency-path: "**/requirements.txt"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Pull Ollama model
        run: |
          # Warte bis Ollama bereit ist
          timeout 60 bash -c 'until curl -s http://localhost:11434/api/tags > /dev/null; do sleep 2; done'

          # Lade das schnellste Modell für CI/CD herunter (llama3.2:1b - nur 1GB RAM, ultra-schnell)
          docker exec $(docker ps -q -f ancestor=ollama/ollama:latest) ollama pull llama3.2:1b

          # Verifiziere, dass das Modell verfügbar ist
          docker exec $(docker ps -q -f ancestor=ollama/ollama:latest) ollama list

      - name: Run all tests (including LLM tests)
        env:
          OLLAMA_BASE_URL: http://localhost:11434
          OLLAMA_MODEL: llama3.2:1b
        run: |
          python -m pytest tests/unit/ tests/integration/ tests/llm/ -v --tb=short
